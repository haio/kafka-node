{"name":"Kafka-node","tagline":"Node.js client for Apache Kafka 0.8","body":"Kafka-node\r\n==========\r\n\r\nKafka-node is a nodejs client with Zookeeper integration for apache Kafka. It only supports the latest version of Kafka 0.8 which is still under development, so this module\r\nis _not production ready_ so far.\r\n\r\nThe Zookeeper integration does the following jobs:\r\n\r\n* Loads broker metadata from Zookeeper before we can communicate with the Kafka server\r\n* Watches broker state, if broker changes, the client will refresh broker and topic metadata stored in the client\r\n\r\n# Install Kafka\r\nFollow the [instructions](https://cwiki.apache.org/KAFKA/kafka-08-quick-start.html) on the Kafka wiki to build Kafka 0.8 and get a test broker up and running.\r\n\r\n# API\r\n## Client\r\n### Client(connectionString, clientId, [zkOptions])\r\n* `connectionString`: Zookeeper connection string, default `localhost:2181/kafka0.8`\r\n* `clientId`: This is a user supplied identifier for the client application, default `kafka-node-client`\r\n* `zkOptions`: **Object**, Zookeeper options, see [node-zookeeper-client](https://github.com/alexguan/node-zookeeper-client#client-createclientconnectionstring-options)\r\n\r\n### close(cb)\r\nCloses the connection to Zookeeper and the brokers so that the node process can exit gracefully.\r\n\r\n* `cb`: **Function**, the callback\r\n\r\n## Producer\r\n### Producer(client)\r\n* `client`: client which keeps a connection with the Kafka server.\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    Producer = kafka.Producer,\r\n    client = new kafka.Client(),\r\n    producer = new Producer(client);\r\n```\r\n\r\n### send(payloads, cb)\r\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   messages: ['message body'],// multi messages should be a array, single message can be just a string\r\n   partition: '0', //default 0\r\n}\r\n```\r\n\r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    Producer = kafka.Producer,\r\n    client = new kafka.Client(),\r\n    producer = new Producer(client),\r\n    payloads = [\r\n        { topic: 'topic1', messages: 'hi', partition: 0 },\r\n        { topic: 'topic2', messages: ['hello', 'world'] }\r\n    ];\r\nproducer.on('ready', function () {\r\n    producer.send(payloads, function (err, data) {\r\n        console.log(data);\r\n    });\r\n});\r\n```\r\n\r\n### createTopics(topics, async, cb)\r\nThis method is used to create topics on the Kafka server. It only work when `auto.create.topics.enable`, on the Kafka server, is set to true. Our client simply sends a metadata request to the server which will auto create topics. When `async` is set to false, this method does not return until all topics are created, otherwise it returns immediately.\r\n\r\n* `topics`: **Array**,array of topics\r\n* `async`: **Boolean**,async or sync\r\n* `cb`: **Function**,the callback\r\n\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    Producer = kafka.Producer,\r\n    client = new kafka.Client(),\r\n    producer = new Producer(client);\r\n// Create topics sync\r\nproducer.createTopics(['t','t1'], false, function (err, data) {\r\n    console.log(data);\r\n});\r\n// Create topics async\r\nproducer.createTopics(['t'], true, function (err, data) {});\r\nproducer.createTopics(['t'], function (err, data) {});// Simply omit 2nd arg\r\n```\r\n\r\n\r\n## HighLevelProducer\r\n### HighLevelProducer(client)\r\n* `client`: client which keeps a connection with the Kafka server. Round-robins produce requests to the available topic partitions\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    HighLevelProducer = kafka.HighLevelProducer,\r\n    client = new kafka.Client(),\r\n    producer = new HighLevelProducer(client);\r\n```\r\n\r\n### send(payloads, cb)\r\n* `payloads`: **Array**,array of `ProduceRequest`, `ProduceRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   messages: ['message body'],// multi messages should be a array, single message can be just a string\r\n}\r\n```\r\n\r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    HighLevelProducer = kafka.HighLevelProducer,\r\n    client = new kafka.Client(),\r\n    producer = new HighLevelProducer(client),\r\n    payloads = [\r\n        { topic: 'topic1', messages: 'hi' },\r\n        { topic: 'topic2', messages: ['hello', 'world'] }\r\n    ];\r\nproducer.on('ready', function () {\r\n    producer.send(payloads, function (err, data) {\r\n        console.log(data);\r\n    });\r\n});\r\n```\r\n\r\n### createTopics(topics, async, cb)\r\nThis method is used to create topics on the Kafka server. It only work when `auto.create.topics.enable`, on the Kafka server, is set to true. Our client simply sends a metadata request to the server which will auto create topics. When `async` is set to false, this method does not return until all topics are created, otherwise it returns immediately.\r\n\r\n* `topics`: **Array**,array of topics\r\n* `async`: **Boolean**,async or sync\r\n* `cb`: **Function**,the callback\r\n\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    HighLevelProducer = kafka.HighLevelProducer,\r\n    client = new kafka.Client(),\r\n    producer = new HighLevelProducer(client);\r\n// Create topics sync\r\nproducer.createTopics(['t','t1'], false, function (err, data) {\r\n    console.log(data);\r\n});\r\n// Create topics async\r\nproducer.createTopics(['t'], true, function (err, data) {});\r\nproducer.createTopics(['t'], function (err, data) {});// Simply omit 2nd arg\r\n```\r\n\r\n## Consumer\r\n### Consumer(client, payloads, options)\r\n* `client`: client which keeps a connection with the Kafka server.\r\n* `payloads`: **Array**,array of `FetchRequest`, `FetchRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   offset: 0, //default 0\r\n}\r\n```\r\n\r\n* `options`: options for consumer,\r\n\r\n```js\r\n{\r\n    groupId: 'kafka-node-group',//consumer group id, deafult `kafka-node-group`\r\n    // Auto commit config \r\n    autoCommit: true,\r\n    autoCommitIntervalMs: 5000,\r\n    // The max wait time is the maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the request is issued, default 100ms\r\n    fetchMaxWaitMs: 100,\r\n    // This is the minimum number of bytes of messages that must be available to give a response, default 1 byte\r\n    fetchMinBytes: 1,\r\n    // The maximum bytes to include in the message set for this partition. This helps bound the size of the response.\r\n    fetchMaxBytes: 1024 * 10, \r\n    // If set true, consumer will fetch message from the given offset in the payloads \r\n    fromOffset: false\r\n}\r\n```\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    Consumer = kafka.Consumer,\r\n    client = new kafka.Client(),\r\n    consumer = new Consumer(\r\n        client,\r\n        [\r\n            { topic: 't', partition: 0 }, { topic: 't1', partition: 1 }\r\n        ],\r\n        {\r\n            autoCommit: false\r\n        }\r\n    );\r\n```\r\n\r\n### on('message', onMessage);\r\nBy default, we will consume messages from the last committed offset of the current group\r\n\r\n* `onMessage`: **Function**, callback when new message comes\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.on('message', function (message) {\r\n    console.log(message);\r\n});\r\n```\r\n\r\n### on('error', function (err) {})\r\n\r\n\r\n### on('offsetOutOfRange', function (err) {})\r\n\r\n\r\n### addTopics(topics, cb)\r\nAdd topics to current consumer, if any topic to be added not exists, return error\r\n* `topics`: **Array**, array of topics to add\r\n* `cb`: **Function**,the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.addTopics(['t1', 't2'], function (err, added) {\r\n});\r\n```\r\n\r\n### removeTopics(topics, cb)\r\n* `topics`: **Array**, array of topics to remove \r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.removeTopics(['t1', 't2'], function (err, removed) {\r\n});\r\n```\r\n\r\n### commit(cb)\r\nCommit offset of the current topics manually, this method should be called when a consumer leaves\r\n\r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.commit(function(err, data) {\r\n});\r\n```\r\n\r\n### setOffset(topic, partition, offset)\r\nSet offset of the given topic\r\n\r\n* `topic`: **String**\r\n\r\n* `partition`: **Number**\r\n\r\n* `offset`: **Number**\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.setOffset('topic', 0, 0); \r\n```\r\n\r\n### close(force, cb)\r\n* `force`: **Boolean**, if set true, it force commit current offset before close, default false\r\n\r\nExample\r\n\r\n```js\r\nconsumer.close(true, cb);\r\nconsuemr.close(cb); //force is force\r\n```\r\n\r\n## HighLevelConsumer\r\n### HighLevelConsumer(client, payloads, options)\r\n* `client`: client which keeps a connection with the Kafka server.\r\n* `payloads`: **Array**,array of `FetchRequest`, `FetchRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName'\r\n}\r\n```\r\n\r\n* `options`: options for consumer,\r\n\r\n```js\r\n{\r\n    groupId: 'kafka-node-group',//consumer group id, deafult `kafka-node-group`\r\n    // Auto commit config\r\n    autoCommitIntervalMs: 5000,\r\n    // The max wait time is the maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the request is issued, default 100ms\r\n    fetchMaxWaitMs: 100,\r\n    // This is the minimum number of bytes of messages that must be available to give a response, default 1 byte\r\n    fetchMinBytes: 1,\r\n    // The maximum bytes to include in the message set for this partition. This helps bound the size of the response.\r\n    fetchMaxBytes: 1024 * 10,\r\n    // If set true, consumer will fetch message from the given offset in the payloads\r\n    fromOffset: false\r\n}\r\n```\r\nExample:\r\n\r\n``` js\r\nvar kafka = require('kafka-node'),\r\n    HighLevelConsumer = kafka.HighLevelConsumer,\r\n    client = new kafka.Client(),\r\n    consumer = new HighLevelConsumer(\r\n        client,\r\n        [\r\n            { topic: 't' }, { topic: 't1' }\r\n        ],\r\n        {\r\n            groupId: 'my-group'\r\n        }\r\n    );\r\n```\r\n\r\n### on('message', onMessage);\r\nBy default, we will consume messages from the last committed offset of the current group\r\n\r\n* `onMessage`: **Function**, callback when new message comes\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.on('message', function (message) {\r\n    console.log(message);\r\n});\r\n```\r\n\r\n### on('error', function (err) {})\r\n\r\n\r\n### on('offsetOutOfRange', function (err) {})\r\n\r\n\r\n### addTopics(topics, cb)\r\nAdd topics to current consumer, if any topic to be added not exists, return error\r\n* `topics`: **Array**, array of topics to add\r\n* `cb`: **Function**,the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.addTopics(['t1', 't2'], function (err, added) {\r\n});\r\n```\r\n\r\n### removeTopics(topics, cb)\r\n* `topics`: **Array**, array of topics to remove\r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.removeTopics(['t1', 't2'], function (err, removed) {\r\n});\r\n```\r\n\r\n### commit(cb)\r\nCommit offset of the current topics manually, this method should be called when a consumer leaves\r\n\r\n* `cb`: **Function**, the callback\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.commit(function(err, data) {\r\n});\r\n```\r\n\r\n### setOffset(topic, partition, offset)\r\nSet offset of the given topic\r\n\r\n* `topic`: **String**\r\n\r\n* `partition`: **Number**\r\n\r\n* `offset`: **Number**\r\n\r\nExample:\r\n\r\n``` js\r\nconsumer.setOffset('topic', 0, 0);\r\n```\r\n\r\n### close(force, cb)\r\n* `force`: **Boolean**, if set true, it force commit current offset before close, default false\r\n\r\nExample\r\n\r\n```js\r\nconsumer.close(true, cb);\r\nconsuemr.close(cb); //force is force\r\n```\r\n\r\n## Offset\r\n### Offset(client)\r\n* `client`: client which keeps a connection with the Kafka server.\r\n\r\n### fetch(payloads, cb)\r\nFetch the available offset of a specify topic-partition\r\n\r\n* `payloads`: **Array**,array of `OffsetRequest`, `OffsetRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   partition: '0', //default 0\r\n   // time:\r\n   // Used to ask for all messages before a certain time (ms), default Date.now(),\r\n   // Specify -1 to receive the latest offsets and -2 to receive the earliest available offset.\r\n   time: Date.now(),\r\n   maxNum: 1 //default 1\r\n}\r\n```\r\n\r\n* `cb`: *Function*, the callback\r\n\r\nExample\r\n\r\n```js\r\nvar kafka = require('kafka-node'),\r\n    client = new kafka.Client(),\r\n    offset = new kafka.Offset(client);\r\n    offset.fetch([\r\n        { topic: 't', partition: 0, time: Date.now(), maxNum: 1 } \r\n    ], function (err, data) {\r\n        // data\r\n        // { 't': { '0': [999] } }\r\n    });\r\n```\r\n\r\n### commit(groupId, payloads, cb)\r\n* `groupId`: consumer group\r\n* `payloads`: **Array**,array of `OffsetCommitRequest`, `OffsetCommitRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   partition: '0', //default 0\r\n   offset: 1,\r\n   metadata: 'm', //default 'm'\r\n}\r\n```\r\n\r\nExample\r\n\r\n```js\r\nvar kafka = require('kafka-node'),\r\n    client = new kafka.Client(),\r\n    offset = new kafka.Offset(client);\r\n    offset.commit('groupId', [\r\n        { topic: 't', partition: 0, offset: 10 } \r\n    ], function (err, data) {\r\n    });\r\n```\r\n\r\n### fetchCommits(groupid, payloads, cb)\r\nFetch the last committed offset in a topic of a specific consumer group\r\n\r\n* `groupId`: consumer group\r\n* `payloads`: **Array**,array of `OffsetFetchRequest`, `OffsetFetchRequest` is a JSON object like:\r\n\r\n``` js\r\n{\r\n   topic: 'topicName',\r\n   partition: '0' //default 0\r\n}\r\n```\r\n\r\nExample\r\n\r\n```js\r\nvar kafka = require('kafka-node'),\r\n    client = new kafka.Client(),\r\n    offset = new kafka.Offset(client);\r\n    offset.fetchCommits('groupId', [\r\n        { topic: 't', partition: 0 } \r\n    ], function (err, data) {\r\n    });\r\n```\r\n\r\n# Todo\r\n* Compression: gzip & snappy\r\n\r\n# LICENSE - \"MIT\"\r\nCopyright (c) 2013 Sohu.com\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of\r\nthis software and associated documentation files (the \"Software\"), to deal in\r\nthe Software without restriction, including without limitation the rights to\r\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\r\nof the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions: \r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}